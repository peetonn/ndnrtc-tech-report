% Template: http://www.acm.org/sigs/publications/proceedings-templates#aL2 
% 
\documentclass{icn/sig-alternate-2013} % {proc}

\usepackage{amsmath}    % need for subequations
\usepackage{graphicx}   % need for figures
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
\usepackage{subfigure}  % use for side-by-side figures
\usepackage{hyperref}   % use for hypertext links, including those to external documents and URLs
\usepackage{url}
\usepackage{multicol}
\usepackage{textcomp}
%\usepackage{caption}  %% supported in class
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
%\usepackage{pstricks}
%\usepackage{authblk}  %% supported in class
% \usepackage{float}
% \newfloat{algorithm}{t}{lop}

\newcommand{\ndnrtcName}{NDN-RTC} % {\emph{ndnrtc}}
\newcommand{\ndnconName}{\emph{ndncon}}
\newcommand{\wConcept}{Interest demand}

%% Shrink lists - http://tex.stackexchange.com/questions/43743/how-to-reduce-line-space-leading-within-an-enumerate-environment 
\setenumerate{itemsep=-0.5ex,topsep=1ex, leftmargin=*}
\setitemize{itemsep=-0.5ex,topsep=1ex, leftmargin=*}

\title{\ndnrtcName{}: Real-time videoconferencing\\ over Named Data Networking}

\numberofauthors{2}
\author{
\alignauthor Peter Gusev\\
       \affaddr{UCLA REMAP}\\
       \email{peter@remap.ucla.edu}
\alignauthor Jeff Burke\\
       \affaddr{UCLA REMAP}\\
       \email{jburke@remap.ucla.edu}
}

\newfont{\mycrnotice}{ptmr8t at 7pt}
\newfont{\myconfname}{ptmri8t at 7pt}
\let\crnotice\mycrnotice%
\let\confname\myconfname%

\permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.}
\conferenceinfo{ICN'15,}{September 30--October 2, 2015, San Francisco, CA, USA. \\ 
{\mycrnotice{Copyright is held by the owner/author(s). Publication rights licensed to ACM.}}}
\copyrightetc{ACM \the\acmcopyr}
\crdata{978-1-4503-3855-4/15/09\ ...\$15.00.\\
DOI: http://dx.doi.org/10.1145/2810156.2810176}

\clubpenalty=10000 
\widowpenalty = 10000

%************************************************
\begin{document}

\maketitle

%************************************************
\abstract
\ndnrtcName{} is a videoconferencing library that employs Named Data Networking (NDN), a proposed future Internet architecture. It was designed to 
provide a platform for experimental research in low-latency, real-time multimedia communication over NDN. It aims to provide an end-user experience similar to Skype or Google Hangouts, while implementing a receiver-driven approach that takes advantage of NDN's name-based forwarding, data signatures, caching, and request aggregation.  As implemented, \ndnrtcName{} employs widely used open source components, including the WebRTC library, VP9 codec, and OpenFEC for forward error correction. This paper presents the design, implementation in C++, and testing of \ndnrtcName{} on the NDN testbed using a demonstration GUI conferencing application, \ndnconName{}, which provides HD videoconferencing over NDN to end-users. 
%************************************************
\section{Introduction}
% About NDN from the standpoint of content distribution.
Named Data Networking (NDN) is a proposed future Internet architecture that shifts the ``thin waist" of the Internet from the current host-centric paradigm of IP to data-centric communication. In NDN, every chunk of data has a name, which is often hierarchical and human-readable, and a cryptographic signature binding name, data, and the key of the publisher.  Consumers of data issue ``Interest" packets for these ``Data" packets by name. Signed, named Data packets matching the Interest can be returned by any node on the network, including opportunistic caches on routers. NDN's intrinsic caching can be leveraged by content distribution applications to reduce the load on data publishers in multi-consumer scenarios~\cite{ndnvideo}. Duplicate Interests for the same content are also aggregated in routers, further reducing the load on those publishers and the network. NDN is described in more detail in publications on the project website\footnote{\url{http://named-data.net}}, including~\cite{ndntechreport, ndntechreport0, ndn-netw}.

% About current low-latency applications and multi-party conferences.
Efficient content distribution has long been a driver application for NDN research and the broader field of Information Centric Networking (ICN). Prior work in this area, including our own, is covered briefly in Section \ref{sec:bg}.  However, low-latency applications, such as ``real-time" videoconferencing, present particular design and implementation issues that have not been as widely explored in publicly available prototypes or the NDN and ICN literature. For example, obtaining the ``latest data'' from a network with pervasive caching, without relying on direct consumer-producer communication (which impacts scaling potential) and while trying to keep application-level latency low, appears to be a significant challenge.

The NDN project team uses application-driven research to explore NDN's affordances for modern applications and to refine the architecture itself.  % About this project and what it does
The \ndnrtcName{} library was created to explore real-time communications (RTC) experimentally. %It was designed, implemented, and evaluated to provide an initial platform for exploring NDN's potential for scalable low-latency audio/video conferencing and ``real-time'' traffic more generally. 
Though it is based on what was learned from our NDNVideo project~\cite{ndnvideo}, \ndnrtcName{} is a clean slate design with new goals.  
The project is ongoing; this paper presents the current design and initial evaluation.  
%\ndnrtcName{} provides basic functionality for publishing audio/video streams, and for fetching these streams with low latency.  This can be leveraged by desktop or web applications, such as the \ndnconName{} sample application, for establishing multi-party conferences. The NDN network's caching and Interest aggregation are leveraged without architectural modification, with the \ndnrtcName{} library providing low-latency communication. 
To work towards the goal of using \ndnrtcName{} in NDN project-related videoconferences and meetings, we needed reasonable CPU and bandwidth efficiency, echo cancellation, and modern video coding performance. Therefore, \ndnrtcName{} is built in C++ for performance and leverages the widely used WebRTC library, incorporating its existing audio pipeline and video codec. 

The remainder of this paper is organized as follows: Section \ref{sec:why} briefly describes potential NDN benefits for real-time communication (RTC). Section \ref{sec:goals} details our goals for \ndnrtcName{}. Section \ref{sec:bg} covers background and prior work. Section \ref{sec:arch} describes the architecture of the library, designed namespace, data structures and algorithms. Section \ref{sec:imp} discusses implementation details. Section \ref{sec:eval} evaluates main outcomes. Finally, Section \ref{sec:conclusion} provides a conclusion and explains future work.


%************************************************
\section{Why use NDN for RTC?}
\label{sec:why}

%A complete exploration of potential benefits for using NDN for real-time communication is outside the scope of this paper. 
Given that NDN proposes a general Internet architecture, we are motivated initially to show its viability for applications beyond the content distribution examples most often discussed in the literature.  However, we can also present a few potential benefits of using NDN for RTC that are exciting for us: 
%% imo, any of the below assumption are not proven/confirmed with tests as for now. these are our expectations from NDN. if I was a reader, I would expect some confirmations of these later in the paper
%Over IP, low-latency audio/video conferencing applications typically establish direct sender-driven peer-to-peer communication channels for media. However,  existing solutions scale poorly to high numbers of producers and consumers without dedicated aggregation units. In these scenarios, they face implementation challenges and inefficiencies related to the connection-based approach currently used in most IP conferencing solutions.  Potential 
\begin{itemize}
\item By using names rather than IP addresses for routing and forwarding, as with any other NDN application, RTC applications stand to inherit the benefits for mobility, scalability, and simplifications of network infrastructure that are currently being researched in the ICN community, a potential boon for RTCs applications. With \ndnrtcName{}, we use a straightforward naming and communication scheme, leveraging conventions where possible, and build on the current libraries, forwarder, and testbed to increase the likelihood that these benefits can be inherited (or at least explored) in future work. 
\item Receiver-driven architectures requiring minimal publisher coordination can gain consumer scalability from the network, which is viable for streaming playout, as shown in past work discussed below. Early tests of \ndnrtcName{}, described in later sections, demonstrate such network-supported scalability for RTC. This suggests that in the future, by using broadcast or group encryption schemes, NDN could efficiently support secure one-to-many or few-to-many low-latency broadcasts with very little additional application infrastructure -- whether of entertainment content, presentations, closed-circuit cameras, computer vision sources, etc. -- in addition to interactive conversations. 
\item Finally, because \ndnrtcName{} builds directly on the thin waist of the NDN architecture, what is learned from exploring low-latency transmission of time series begins to provide broadly applicable insight into handling other high-rate and/or low-latency time series, such as sensor data feeds.  
\end{itemize}

%************************************************
\section{Design Objectives}
\label{sec:goals}
As discussed above, \ndnrtcName{} aims to explore low-latency audio/video communication over NDN, and to support a working multi-party conferencing application that can be used by NDN project team members across the existing NDN testbed. It also aims to preserve network-supported scalability by avoiding direct consumer-producer communication (i.e., Interests that cannot be aggregated). Further, it explores if ``real-time'' communication can be achieved in a manner consistent with most other NDN data dissemination applications (described at a high level, as follows). 

    Applications using the library implement bidirectional communication by acting as a publisher of their own media streams and consumer of others. A publisher 1) acquires and transforms media data, 2) names, packetizes and signs it, and then 3) passes the packets to an internal or external component that responds to Interests received from the ``black box'' of the NDN network with signed, named data chunks. A consumer issues Interests with appropriate names and selectors to that ``black box'' of the NDN network, at the rate necessary to achieve its objectives and be a good citizen of the network\footnote{While work on congestion control and defining proper behaviour of such applications on the network is underway, it is future work with respect to this paper.}--as informed by the performance of the network it observes in response to its requests. It reassembles and renders them.  Rate adaptation can be handled at the consumer by publishing in multiple namespaces corresponding to multiple bitrates or to layers of a scalable video stream and enabling the consumer to select them on-the-fly.

Once the namespace is defined, the publishing problem in this scenario (and in practice, at least so far) is relatively straightforward.  Complexity is at the the consumer, which must determine what names to issue at what rate, to get the best quality of experience for the application.  For real-time conferencing, this means low-latency access to the freshest data that the ``black box'' of the NDN network can deliver to a given consumer.  

Conference setup and multi-party chat could be handled by applying techniques such as those developed in ChronoChat~\cite{chronochat}, which uses set reconciliation-based synchronization protocols to exchange messages in a many-to-many scenario. 

Based on this high-level concept, specific design goals were developed for the  \ndnrtcName{} library:

%% JB - I took away the future work here, and emphasized that most of these things lay groundwork for future experimentation.  Let's reiterate this throughout. 

\begin{itemize}

\item \textbf{Low-latency audio/video communication.} The library should be capable of maintaining low-latency (approx. 250-750ms) communication for audio and video, similar to consumer videoconferencing applications.

\item \textbf{Multi-party conferencing.} Publishing and fetching several media streams simultaneously should be straightforward.

\item \textbf{Passive consumer \& cacheability.} There should be no explicit negotiation or coordination between publisher and consumer for the media transmission itself, to enable exploration of network-supported scaling to very high consumer to producer ratios.  

\item \textbf{Multiple bitrates.} The library and namespace should support multiple bitrates (from which consumers will select), enabling near future
work on adaptive rate control. 

\item \textbf{Data verification.} The library should provide content verification using existing NDN features, as a building block for trust management and encryption-based access control.  

\end{itemize} 

%% JB: Added this to address several reviewer's comments about why we got down to the segment level, and why our assumptions about the network are simplistic.  We probably want to refine / reiterate throughout. 

Our design approach was further influenced by the relatively young state of NDN research: 

\begin{itemize} 

\item \textbf{Segment-level control.}  At the protocol level, \ndnrtcName{} works directly with data segments rather than video frames, group-of-pictures (GOP) blocks, or other higher-level constructs that it uses at the application level.  %(like RTT, total number of segments per frame, etc.).
This choice was made because most abstractions for most Interest-Data exchange explored in current research and available implementations do not handle low-latency, deadline-driven playout, assume large buffer sizes relative to network latency variations, and because frame sizes often exceed current NDN data object maximum sizes. 
While in the future, successful fetching and buffering patterns may be abstracted to a lower-level library, this approach enabled us to experiment with Interest expression and buffering at fine granularity. For example, \ndnrtcName{} uses per segment metadata that can be exploited by the consumer to adjust fetching behavior. %Note that where appropriate, application-level operations are indeed performed on frames, such as forward error correction based recovery. 

\item \textbf{Assumptions about the network.}  Throughout the paper are a number of assumptions about caching performance and other network behavior. Although future networks may have more complex behavior, the intention here is to explore the performance of basic assumptions on the NDN testbed, rather than proposing schemes to address the emergent behavior of complex ICN networks.

\end{itemize}

%In these ways we hope that \ndnrtcName{} presents not the endgame for RTC over NDN but rather offers an experimental platform for developing more refined approaches. 

%************************************************
\section{Background and prior work}
\label{sec:bg}

%% JB: Shorten!

Most video streaming work on ICN has focused on playout without the constraints of supporting interactive conversations.  For example, UCLA's NDNVideo, which supported live video and playback, was tested and deployed over NDN \cite{ndnvideo}, scaling to approximately one thousand consumers from a single, simple publisher over plain-vanilla NDN.~\cite{CrowleyNEAPresentation} Its data chunks were named sequentially according to NDN naming conventions \cite{ndnnaming}, with a second namespace mapping sequence numbers to a timeline to enable efficient time-based random access by consumers. Though the project worked well for live and pre-recorded media streaming, it did not meet requirements for low-latency communication in its Interest pipelining approach. Also, its media architecture, based on GStreamer, was not immediately extensible for use as a conferencing solution. Subsequent work at UCLA, NDNlive and NDNtube \cite{ndnlive-tube} demonstrated a new API for application developers, the Consumer/Producer API \cite{cons-prod-api}, which works with higher-level ``Application Data Units" rather than Interests/Data packets.  To our knowledge, that work also does not target or achieve RTC, and like NDNVideo, is built on GStreamer, which does not easily support the audio pipeline needed for interactive conversations. %They functioned over with the latest NFD (NDN Forwarder Daemon) and use repo-ng \cite{repo-ng} for offline media storage. At the time of publication of current work NDNlive was still under development, and the project does not aim to address real-time audio/video conferencing.  
Other streaming video work includes \cite{ccn-dash} and \cite{ndn-dash}, which explore the advantages of using ICN networks for MPEG Dynamic Adaptive Streaming over HTTP (DASH). Although not directly related to low-latency streaming, these works also leverage ICN networks' caching ability for serving chunks of video files efficiently to multiple consumers.
In contrast with the above, \ndnrtcName{} was developed from the ground up with low     latency and interactive conversations in mind. %, providing an alternative design motivated by these requirements. %Further, as described below it leverages the complete WebRTC audio pipeline to provide echo cancellation and other features that are necessary for interactive conversations. 

%\ndnrtcName{} is also distinct from previous efforts in real-time communication. 
The Voice-over-CCN project \cite{voccn} was an early exploration of real-time communication over ICN, providing a similar level of quality compared to VoIP solutions with much greater scalability potential and simpler, more flexible architecture. VoCCN introduced pipelining Interests in a real-time scenario, an idea also employed in \ndnrtcName{}. Subsequently, an audio conferencing application, ACT, was developed early in the NDN project \cite{act-tool}. It leveraged use of the Mumble library and successfully used NDN as a transport. Efforts for conference and user discovery were made in this work as well. %, which was continued in ChronoChat (mentioned above).  
However, echo cancellation quality was poor, which made it difficult to use, and only preliminary work in video was performed. This led us to build \ndnrtcName{} on top of the WebRTC library, despite the additional, significant implementation complexity, in order to use its audio-processing capabilities and video codecs, and potentially give an opportunity for easier integration with supported web browsers.\footnote{No  modifications are needed to WebRTC \cite{webrtc}  as used in \ndnrtcName{}, which we believe will enable us to make comparisons between IP and NDN implementations of the library in the future.}  Finally, the most recent related work in ICN-based real-time communication, of which we are aware, is \cite{huawei-rtc}; however, as we understand, it currently handles Interest retransmission and buffering at the GOP level and does not (yet) meet our latency requirements.

%%which made it difficult to use in real world scenarios. what is "it"?
% cite any others? 
% Need to also cite the buffer fill paper, etc. 

%************************************************
\section{Application Architecture}
\label{sec:arch}

There are two roles in the \ndnrtcName{} library: producer and consumer.  In bidirectional communication, applications use the library to play both roles, but a variety of other multi-party and one-to-many scenarios can be achieved.  The library is built on a consumer-driven approach directly following NDN's Interest-Data exchange model.  In contrast to the sender-driven approach of typical IP-based RTC, the producer publishes data to network-connected storage at its own pace, while the consumer requests data as needed and manages the relationship between outgoing Interests, incoming data segments, and buffer fill. 
%%In the application????
% Redundant figure
%
%\begin{figure}[t!]
%\centering
%\includegraphics[width=0.5\textwidth]{architecture}
%\caption{RTC over NDN}
%\label{fig:arc}
%\end{figure}

% this paragraph describes figure which was considered redundant
%Figure \ref{fig:arc} presents a top-level overview of how \ndnrtcName{} works. Local media capture and cache belong to the producer. Media is stored in the cache which provides access to the data for all incoming Interests. Remote playback represents the consumer: issues Interests, prepares received media (assembles video frames from segments and re-orders them) and plays it back.

\subsection{Producer}
The producer's main tasks are to acquire video and audio data from media inputs, encode them, marshal the data into packets, \textit{name} those packets, sign them, and store them in an application-level cache\footnote{Currently, this cache is provided to the application by the NDN-CCL library.} that will asynchronously respond to incoming Interests. Flow control responsibility is shifted to the consumer, and scaling is supported by network caches downstream rather than publishing infrastructure at the application level.  

%************************************************
\subsection{Namespace}

% Why is namespace design a research question?  what's tricky? 
A primary design question is how the data should be named so it can be retrieved by the consumer with the desired properties. The \ndnrtcName{} namespace defines names for media (segmented video frames and bundled audio samples), error correction data, and metadata, as shown in Figure~\ref{fig:namespace}.  The namespace is designed to efficiently support consumer-driven communication, as introduced in Section~\ref{sec:arch} and detailed in Section~\ref{sec:consumer}. Note that \ndnrtcName{} handles audio and video streams independently, which enables the library to support audio-only streaming, and for applications to prioritize audio over video for increased quality of experience for interactive conversations in bursty or low-bitrate scenarios.  
%Besides that, the namespace should also reflect data specialization hierarchy - from general concepts in the root to more specialized entities towards the leaves. 

\subsubsection{Media} 

\ndnrtcName{} employs the abstraction of a \textit{media stream}, which describes a flow of one type of media, such as video frames or audio samples, coming from a source--currently, an input device on the producer.
%(For now, names for streams are derived from their corresponding device information.) 
A typical publisher will publish several media streams simultaneously--e.g., video from a camera, audio from a microphone, video from screen capture.  The data from a stream may be encoded at one or more bitrates, so in the name hierarchy, each stream has children corresponding to different encoder instances called \textit{media threads}. Media threads allow the producer to, for example, provide the same media stream in several quality levels, such as low, medium and high, so that the consumer can choose the one suitable for its requirements and current network conditions.

%\begin{figure}[t!]
%\centering
%\includegraphics[width=0.5\textwidth]{streams-hierarchy}
%\caption{\ndnrtcName{} media streams hierarchy}
%\label{fig:stream-hierarchy}
%\end{figure}
\ndnrtcName{} packetizes the WebRTC video encoder output directly. Encoded media segments published under the hierarchical names described above, with video frames further separated into two namespaces per frame type, \texttt{delta} and \texttt{key}, each numbered sequentially and independently. (Section~\ref{sec:eval} elaborates more on the reasons why this separation is needed.) 
%Unlike Key frames, Delta frame decoding depends on previous Key frame and all Delta frames that come after it. 
%For reasons described in Section~\ref{sec:eval}, the namespace distinguishes in this way between two types of encoded video frames, key and delta frames. Key frames do not depend on any previous frames to be decoded. Delta frames are dependent on the previous frames (received after the last key frame), and cannot be decoded without significant visual artifacts if any of the key frames are missing. 

%%Encoded media segments published under the hierarchical names described above, with video frames further separated into two namespaces per frame type, \texttt{delta} and \texttt{key}, each numbered sequentially and independently. ---- This sentence needs to be reworded.

The next level in the tree separates data by type, either media or parity. Parity data for forward error correction, if the producer opts to publish it, can be used by a consumer to recover frames that miss one or more segments. 
%% What was motivation for separating FEC naming this way and not some other way? 
%% PG: i believe it's just implementation - having parity data named similar to media gives the advantage of using existing fetching/assembling algorithms on the consumer side
In the case of both parity and regular frames, the deepest level of the namespace defines individual data segments. Any media packet, except audio, consists of one or more segments.\footnote{For example, the average sizes of frames for 1000 kbps stream using VP8/VP9: key frames are $\approx$ 30KB, and delta frames are $\approx$ 3-7KB. Therefore, depending on the underlying transport's performance for delivering objects of this size, the producer may need to segment encoded frames into smaller chunks and name them in a way that makes reassembly straightforward. Based on our current observations of performance and the prevalence of UDP as a transport for the NDN testbed, \ndnrtcName{} currently packetizes media into segments that are less than the typical 1500 byte MTU.} These segments are numbered sequentially, and their names conform to NDN naming conventions \cite{ndnnaming}.

Audio stream samples are much smaller than the maximum payload size, and there is no equivalent to the key/delta frame distinction in the audio codecs in use. Therefore, all audio packets are published under the \texttt{delta} namespace. Multiple audio samples are bundled into one data packet, until the size of one data segment is reached, and published only after that.
%% How is audio/video sync maintained?? 
%% PG: there are timestamps included in metadata described in the next sections which are used for AV sync

\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{namespace}
\caption{\ndnrtcName{} namespace}
\label{fig:namespace}
\end{figure}

\subsubsection{Metadata} 

\ndnrtcName{} uses both stream-level and packet-level metadata. 
%Consumers need to know the producer's specific namespace structure in order to fetch data successfully. 
Consumers need to know the producer's publishing configuration and, to save them from traversing the producer's namespace, the producer publishes meta-information about current streams under \texttt{session info} and updates it whenever the configuration has changed.
%Thus, consumers can retrieve current information about the producer's publishing state.

Additionally, data names carry further metadata as part of each packet, which can be used by consumers regardless of which frame segment was received first. 
Four components are added at the end of every data segment name:
%% should really use texttt convention for names, I think
\small\begin{equation}
\texttt{\textit{.../delta}/\textit{frame\_no}/\textit{data}/\textit{seg\_no}/\textbf{$n_{seg}$}/\textbf{$pl$}/\textbf{$pr_{seq}$}/\textbf{$par_{num}$} \nonumber}
\end{equation}\normalsize
\begin{itemize}[label={}]
\item \texttt{$n_{seg}$} - total number of segments for this frame;
\item \texttt{$pl$} - absolute playback position for current frame (this is different from the \textit{frame}, which is a sequence number for the frame in its domain, i.e. \texttt{key} or \texttt{delta});
\item \texttt{$pr_{seq}$} - sequence number of the corresponding frame from other domain (i.e., for delta frames, it is the sequence number of the corresponding key frame required for decoding);
\item \texttt{$par_{num}$} - number of parity segments for the frame.
\end{itemize}
Metadata in the name, rather than in the packet, is expected to be useful for application components or services that may not need to understand the packet payload. 

%************************************************
\subsection{Data objects}
The producer generates signed data objects from input media streams and places them into an in-memory, application-level cache.  These objects contain stream data and metadata.

\subsubsection{Media stream}
% just explained above
%Each media packet payload consists of two types of data -- a chunk from the media stream and metadata that describes it. 
Video stream data contains raw bytes received from the WebRTC library's video encoder. For audio, \ndnrtcName{} captures and encapsulates RTP and RTCP packets coming from the WebRTC audio processing pipeline, in order to obtain echo cancellation and gain control and other features, which are then fed into a similar pipeline on the consumer side for proper rendering and corrections.\footnote{This is an artifact of the current implementation to benefit from the full audio pipeline of WebRTC, which is difficult to unbundle from RTP.}

\begin{figure}[t!]
\centering
\subfigure[Video frame segmentation.]{\label{fig:segment}\includegraphics[width=0.5\textwidth]{segmentation}}
\subfigure[Audio sample bundling.]{\label{fig:audio-bundling}\includegraphics[width=0.3\textwidth]{audio-bundling}}
\caption{Segmentation and bundling}
\label{fig:packetmeta}
\end{figure}

\subsubsection{Metadata}

%The receiver-driven architecture of \ndnrtcName{} and our approach's deliberate avoidance of explicit consumer-producer synchronization have suggested the importance of providing sufficient meta information from the producer. 
Apart from metadata provided in the namespace (as was described earlier), there is also metadata supplied in the data objects themselves.
%The most critical and potentially most useful for a variety of applications and services is provided in the namespace, as described above.  Other information is provided in the data objects themselves. Such separation is currently experimental and provides some benefits, e.g., quickly being able to retrieve the total number of segments in the frame without the need for content decoding or knowledge of the exact payload format.
%  You really should verify the name... so I took avoiding verification out
%
Every media sample is prepended with frame-level metadata, the \textit{frame header} (see Figures \ref{fig:packetmeta} and \ref{fig:data-struct}), which carries encoding and timing information. %Each audio bundle (packet) is prepended by similar frame header, as seen in Figure \ref{fig:audio-bundling}. 
%There are two header types: \textit{frame} and \textit{segment}. 
%The frame header contains media-specific information such as frame size, timestamp, current rate and Unix timestamp\footnote{Producer timestamp is \textit{not} required but can be used for calculating actual delay between NTP-synchronized producers and consumers)} (see Figure \ref{fig:data-struct}).
%Additionally, as an aid for experimentation and for optimizing two-way conversations, the segment header also 
Another type of metadata, the \textit{segment header}, is appended to individual segments and carries the producer's observations of Interest arrival. This information is used by consumers to adjust fetching and playback mechanisms.  
%% not sure how experimental is that, because consumers rely on generation delay to estimate RTT, thus adjust lambda, buffer, etc..
Segment headers make use of the Interest nonce value, and thus may not be as useful in larger multi-party calls. At this time, they are used primarily for experimental and evaluation purposes:
\begin{itemize} [label={}]
\item \textit{Interest nonce}: Nonce of the Interest \textit{first} received at the publisher for a particular segment. Example interpretations include: 1) Value belongs to an Interest issued previously: Consumer received non-cached data requested by previously issued Interest; 2) Value is non-zero, but it does not belong to any of the previously issued Interests: Consumer received data requested by some other consumer; data may be cached; 3) Value is zero: Data requested after it has been produced; data is cached.
\item \textit{Interest arrival timestamp}: Timestamp of the first Interest's arrival at the producer. Monitoring publisher arrival timestamps may give the consumer that issued the Interest information about how long it takes for Interests to reach the producer. %This value is only valid when the nonce value belongs to one of a given consumer's Interests.
\item \textit{Generation delay}: Time interval in milliseconds between Interest arrival and segment publishing. If the nonce is its own, a consumer can use this value in order to control the number of outstanding Interests. %This value is again only valid when the nonce value belongs to one of the consumer's Interests.
\end{itemize}


%% The following paragraphs is relocated and was unclear

% This one seems to duplicate the above- 
%
%Such information (which could include Interests' nonce values, Interests' arrival timestamps and data generation delays), if added to the returned data segment, may help consumers evaluate relevant network performance, detect congestion, and assess whether incoming data is likely to be stale (delayed beyond the path delay). 

% This seems to either be future work, or on the consumer side.  It is
% unclear whey it was above
%
%Furthermore, keeping historical data on the consumer side may help Interest pipelining in the future. For instance, providing the average number of segments per frame type helps consumers estimate the number of required initial Interests to fetch upcoming frames. This helps keep frame fetching cycles minimal.


\begin{figure}[t!]
\centering
\includegraphics[width=0.3\textwidth]{data-struct}
\vspace{-4pt}
\caption{Frame and (experimental) segment headers.}
\label{fig:data-struct}
\end{figure}


%************************************************
\subsection{Consumer}
\label{sec:consumer}

In \ndnrtcName{}'s receiver-driven architecture, the consumer aims to 1) choose the most appropriate media stream bandwidth from those provided by the producer, e.g., by monitoring network conditions; 2) fetch and, if necessary, reassemble media in the correct order for playback; 3) mitigate, as far as possible, the impact of network latency and packet drops on the viewer's quality of experience.  
The consumer implements Interest pipelining and data buffering, as shown in Figure \ref{fig:consumer}. An asynchronous Interest pipeline issues Interests for individual segments. Independently, a frame buffer handles re-ordering of packets, and informs the pipeline of its status to prompt Interest reexpression.

\begin{figure}[t!]
\centering
\subfigure[Producer]{\label{fig:producer}\includegraphics[width=0.4\textwidth]{producer}}\qquad
\subfigure[Consumer]{\label{fig:consumer}\includegraphics[width=0.4\textwidth]{consumer}}
\caption{\ndnrtcName{} producer and consumer operation.}
\end{figure}

%************************************************
\subsubsection{Frame fetching}

%The consumer obtains the number of segments per frame from metadata in the received segment; however, to minimize latency, it  issues a pipeline of Interests simultaneously. 
The consumer uses an estimate of the number of segments it must fetch for a given frame, issuing $M$ initial Interests, as illustrated in Figure \ref{fig:pull}. If Interests arrive too early, they will be held in the producer's PIT and stay there until the frame is captured and packetized. The delay between Interest arrival and availability of the media data is called the \textbf{generation delay}, $d_{gen}$. Conceptually, this interval should be kept low, to avoid accumulating outstanding Interests with short lifetimes; however,  Interests should not arrive after data is published, as this increases latency from the end-user's perspective.  Once the encoded frame is segmented into $N$ segments and published, Interests $0 - M$ are answered, and the Data returns to the requestor(s). 

Upon receiving the first Data segment, the consumer knows from the metadata the exact number of segments $N$ for the current frame, and issues $N - M$ more Interests for the missing segments, if any. These segments will be satisfied by data with no generation delay, as the frame has been published already by the producer. The time interval between receiving the very first segment and when the frame is fully assembled is represented by $d_{asm}$ and called \textbf{assembly time}. Note that for frames that have less segments than the estimate, some Interests may go unanswered.%; this is currently a tradeoff made to try to keep latency low for the frame as a whole. %These Interests have low lifetimes, of about 300ms. 
 

\begin{figure}[t!]
\centering
\includegraphics[width=0.5\textwidth]{frame-fetch}
\vspace{-18pt}
\caption{Data retrieval timeline.}
\label{fig:pull}
\end{figure}

%Of course, additional round trips for requesting missing data segments increase overall frame assembly time and the possibility that the frame will be incomplete by the time it should be played back. 
%Additional round-trips to refetch missing data can be avoided if the consumer is able to make more accurate estimates of the number of initial Interests. 
More accurate estimates of the number of initial Interests per frame can help avoid additional roundtrips.
The library estimates the average number of segments for each frame type for a given bitrate, and uses this evolving estimate to calculate the number of initial Interests to issue for the next frame in each namespace. %, as the average number of segments varies greatly for the two frame types. Additionally, the consumer tracks the average number of segments per frame type, adapting its estimates over time.

A similar process is used for fetching audio, though for now, audio bundles are carried by just one segment.

%% adressing review 60A-1 (why not to fetch the whole frame?)
%The presence of valuable interest-level metadata in each data chunk requires consumer to operate on segment level and makes it complicated to abstract fragmentation to a lower layer (i.e. Consumer-Producer API \cite{cons-prod-api}).

\subsubsection{Buffering}

% Buffer:
% - re-ordering
% - added latency to mitigate network delays
% - extended defition:
% 	- pending frames
% 	- assembling frames
% - buffer-based retransmissions

The consumer uses a \textit{jitter buffer} to manage out-of-order data arrivals and variations in network delay, and as a place to assemble segments into frames (see Figure \ref{fig:old-buf}). %However, the role of such a buffer  has some NDN-specific aspects. In sender-driven video delivery, buffer slots can be allocated per segment using sequential numbering. 
Our receiver-based paradigm requires the consumer to request data by name explicitly, and organize it by frame as well as segment. %Therefore, after expressing an Interest, the consumer ``knows" that new data is coming, and a named frame slot can be reserved in the buffer, though the number of segments is not known. Practically, this means that there will always be some number of reserved empty slots in the buffer. 
Outstanding Interests are represented in the buffer by ``reserved slots" - those that have partial frame data or no data at all.
The \ndnrtcName{} jitter buffer's size is expressed in terms of two values measured in milliseconds at any given point in time. Its \textit{playback size} is the playback duration in milliseconds of all complete ordered frames; its \textit{estimated size} is \textit{playback size} + \textit{number of reserved slots} $\times$ 1/\textit{producer rate}, which reflects the estimated size of the buffer when all reserved slots have data. Each frame-level slot has an associated set of interests. 
The difference between estimated buffer size and playback size corresponds to the effective RTT, called $RTT^{\prime}$ (this cannot be smaller than the actual network RTT value). %Minimal $RTT^{\prime}$ value indicates that consumer receives the most recent data for the least amount of outstanding Interests. %% NOT CLEAR WHAT THIS MEANS 
%% PG: actually it's not entirely true - minimal RTT' could also mean that consumer is getting cached data...
%Monitoring this value over times provides the consumer information on possible ``sync" status with the producer as will be described further below. %% HOW?. PG: described futher 
%For example, the consumer may use it during the fetching process, as will be discussed in the next section. 
%% Correct formatting for($RTT^{\prime}$)?

Playout progress of the jitter buffer is used for retransmission control. %As shown in Figure \ref{fig:old-buf}, 
At $J$ milliseconds from the buffer end there is a checkpoint, after which it is estimated to be too late for another round trip. When a frame reaches the checkpoint, it is checked for completeness. If the frame is incomplete and cannot be recovered using available parity data, Interests for the missing segments are re-issued. 

\subsubsection{Interest expression control}

%%  Probably want to cover all cases of interest rexpression here
%% - Prompted by timeouts in buffer
%% - To do the chasing

%This section explores current mechanisms for Interest expression and re-expression in more detail.  
A key challenge of a consumer-driven model for videoconferencing, in a caching network, is how to ensure the consumer acquires the latest data without (per our design goal) resorting to direct producer-consumer communication.
To get fresh data, the consumer cannot rely on flags in the protocol, such as \textit{AnswerOriginKind} and \textit{RightMostChild}. The frame period for streaming video is of the same order of magnitude as network round-trip time, suggesting there is no guarantee that the data satisfying those flags will be the most recent data received by the consumer. Instead, it is necessary to use other indicators to ensure that the consumer is requesting and receiving the most up-to-date stream data possible given its (potentially evolving) network connectivity. 

Our current solution is to leverage the known sample publishing rate, which is available in stream-level metadata, and note that, under normal operation, old, cached samples are likely to be retrieved more quickly than new data.~\footnote{If the consumer is the \emph{only} consumer of the stream, its Interests will go directly to the publisher, which also yields the correct behavior. A more complex challenge, for further study, is when segments are inconsistently cached in different ways along the path(s) that Interests take.} We define the \textbf{interarrival delay} ($d_{arr}$) as the time between receipt of successive samples by a given consumer. 

The library currently assumes that delays in the most recent samples follow the publishers' generation pattern, but older, cached data will follow the pattern of Interest expression.\footnote{Though this assumption has proved successful in tests so far, we acknowledge more work is required to address more complex network conditions.} Therefore, by monitoring inter-arrival delays of consecutive media samples and comparing them to the timing of its own Interest expression, which is distinct from the expected generation pattern, consumers can estimate whether they are receiving fresh data or cached data (see Figure \ref{fig:inter-arrival}). The consumer's objective is to obtain fresh data at a consistent rate from the network as a black box, not for Interests to ``reach'' the producer directly.  

%%The following sentences really don't make much sense: To get fresh data, which can be cached but should not be the newest available for the consumer's path, the consumer cannot rely only on using such flags as \textit{AnswerOriginKind} and \textit{RightMostChild}. The high frequency nature of streaming data makes no guarantees that the data satisfying those flags received by a consumer will be the most recent one.


\begin{figure}[t!]
\centering

\subfigure[Bursty arrival of cached data, which reflects Interests expression pattern and indicates that the data is not the latest.]{\label{fig:cached}\includegraphics[width=0.4\textwidth]{arrival-cached}}\\
\subfigure[Periodic arrival of fresh data, reflects publishing pattern and sample rate.]{\label{fig:fresh}\includegraphics[width=0.35\textwidth]{arrival-fresh}}

\caption{Getting the latest data: arrival patterns for the cached and most recent data}
\label{fig:inter-arrival}
\end{figure}

\ndnrtcName{} interest expression is managed in two modes, \textit{bootstrapping} and \textit{playback}.  %The \textit{bootstrapping mode} is active when a consumer first initiates data fetching and tries to exhaust cached data by changing the number of outstanding Interests. After the consumer has exhausted the cache, it switches into the \textit{playback mode}, described further below. 
During bootstrapping, the consumer ``chases" the producer and aims to exhaust network cache of historical (non-real time) segments. By increasing the number of outstanding Interests, the consumer ``pulls cached data" out of the network, unless the freshest data begin to arrive.  In order to control Interest expression, the \ndnrtcName{} consumer tracks a quantity called ``\wConcept{}",  $\lambda$, which can be interpreted as how many outstanding Interests should be sent at the current time (see Figure \ref{fig:w-concept}). The consumer expresses new Interests when $\lambda > 0$. For example, before the bootstrapping phase, the consumer initializes $\lambda$ with a value which reflects the consumer's estimate of how many Interests are needed in order to exhaust network cache and reach the most recent data. In playback, every time a new Interest is expressed, $\lambda$ is decremented, and when new data arrives, $\lambda$ is incremented, thus enabling the consumer to issue more Interests.\footnote{While inspired by the TCP congestion window, the \wConcept{}, as currently employed in \ndnrtcName{}, may play a different role in ICN networks, which we are exploring experimentally in this application.} 

\begin{figure}[t!]
\centering

\subfigure[``\wConcept{}", $\lambda$]{\includegraphics[width=0.35\textwidth]{w-concept}}
\subfigure[Interest bursting ($\lambda+3$)]{\label{fig:int-burst}\includegraphics[width=0.35\textwidth]{int-burst}}
\subfigure[Interest withholding ($\lambda-3$)]{\label{fig:int-hold}\includegraphics[width=0.35\textwidth]{int-hold}}

\caption{Managing Interest expression}
\label{fig:w-concept}
\end{figure}

\textbf{Bootstrapping.} In the current design, there are two experimentally determined indicators that are used by the consumer to adjust $\lambda$: effective $RTT$ ($RTT^\prime$) and inter-arrival delay $d_{arr})$. As described above, at bootstrapping (and re-acquisition), the consumer interprets $d_{arr}$ stabilization around a relatively constant period, in order for the consumer to receive the freshest data available from the network. However, this does not necessarily ensure that the consumer issues Interests efficiently. Figure \ref{fig:ws10} displays that although the consumer has exhausted the cache rather quickly, $RTT^\prime$ is three times larger than the actual $RTT$ for the network (100ms), which means that the majority of the issued Interests remain pending while waiting for the requested data to be produced.

\begin{figure}[t!]
\centering
%\captionsetup[subfigure]{aboveskip=-1pt,belowskip=-2pt}
\begin{scriptsize}
\subfigure[$\lambda=10$: short chasing, larger $RTT^\prime$]{\label{fig:ws10}\def\svgwidth{0.48\textwidth}\input{w10.pdf_tex}}
\subfigure[$\lambda=4$: longer chasing, smaller $RTT^\prime$]{\label{fig:ws4}\def\svgwidth{0.48\textwidth}\input{w4.pdf_tex}}
\subfigure[$\lambda=3$: consumer can't exchaust cache, $RTT^\prime = RTT$]{\label{fig:ws3}\def\svgwidth{0.48\textwidth}\input{w3.pdf_tex}}
\end{scriptsize}
\caption{Larger $\lambda$ decreases ``chasing" phase, but increases $RTT^\prime$ for the same network configuration ($RTT\approx100ms$)}
\label{fig:ws}
\end{figure}

The consumer makes several iterative attempts to adjust $\lambda$ during bootstrapping, which can be described as follows:
\begin{enumerate}
\item The consumer initializes \wConcept{} with $\lambda_d$, and initiates Interests expression.
\item If the consumer did not receive freshest data during the allocated time\footnote{In the current implementation, 1000ms.}, it increases \wConcept{}: $\lambda=\lambda+0.5\lambda_d; \lambda_d = \lambda_d+0.5\lambda_d$.
\item \label{decrease-w} Whenever the consumer receives data determined to be fresh (cache exhausted), it decreases \wConcept{}: $\lambda=\lambda-0.5\lambda_d; \lambda_d = \lambda_d-0.5\lambda_d$ and waits for one of two results:
a) $RTT^\prime$ decreases and the consumer still receives the freshest data -- repeat step \ref{decrease-w};
b) $d_{arr}$ fluctuates unexpectedly, indicating cached data -- restore previous value for $\lambda_d$, increase $\lambda$ accordingly and stop any further adjustments as the consumer has achieved sufficient synchronization with the producer.
\end{enumerate}

%% addressing review 60D-6
Note that $\lambda$ is a counter of how many Interests can be issued more whereas $\lambda_d$ represents the total number of outstanding Interests allowed. The value $\lambda_d-\lambda$ shows how many outstanding Interests consumer has issued at any given point in time. The way $\lambda$ and $\lambda_d$ are adjusted was determined empirically and may be a good topic for further research, along with how often to re-check that the consumer is obtaining the latest data through the steps above. 

Bootstrapping begins with issuing an Interest with the enabled \textit{RightMostChild} selector, in \texttt{delta} namespace for audio and \texttt{key} namespace for video (the video decoding process can start only with a key frame). %The reason this process differs for video streams is that the consumer is not interested in fetching delta frames without having corresponding key frames for decoding. 
Once an initial data segment of a sample with number $S_{seed}$ has been received, the consumer initializes $\lambda$ with initial value $\lambda_d$, and asks for the next sample data $S_{seed}+1$ in the appropriate namespace. Upon receiving the first segments of sample $S_{seed}+1$, the consumer initiates the fetching process (described above) for all namespaces (\texttt{delta} and \texttt{key}, if available). The bootstrapping phase stops when the consumer finds the minimal value of $\lambda$, which still allows for receiving the most recent data, and the consumer switches to the playback mode.

\wConcept{} provides a manageable mechanism to speed up or slow down Interest expression, coupling the asynchronous Interest expression mechanism with the status of the playback buffer. An increase in $\lambda$ value makes the consumer issue more Interests (Figure \ref{fig:int-burst}), whereas any decrease in $\lambda$ holds the consumer back from sending any new Interests (Figure \ref{fig:int-hold}). Larger values of $\lambda$ make the consumer reach a synchronized state with the producer more quickly. However, a larger value means a larger number of outstanding Interests and larger $RTT^\prime$ because of longer generation delays $d_{gen}$ for each media sample. By adjusting the value of $\lambda$ and observing inter-arrival delays $d_{arr}$, the consumer can find minimal $RTT^\prime$ value while still getting non-cached data, adapting towards a loose synchronization with the producer.

\textbf{Playback.}  During playback, the consumer continues to observe $RTT^\prime$ and $d_{arr}$. Whenever $d_{arr}$ indicates that no fresh data is being received, the consumer increases \wConcept{} and starts the adjusting process over again to find minimal $RTT^\prime$ for the new conditions. Such an approach helps the consumers to adjust in cases when data may suddenly start to arrive from a different network hub which introduces new network $RTT$.

\textbf{Interest batches.} Practically, for video, the consumer controls expression of ``batches" of Interests rather than individual Interests, because video frames are composed of several segments.  $\lambda$ is adjusted on a per-frame basis, rather than per-segment. 

\section{Implementation}
\label{sec:imp}
\ndnrtcName{} is implemented as a library written in C++, which is available at \url{https://github.com/remap/ndnrtc}. 
It provides a publisher API for publishing an arbitrary number of media streams (audio or video) and a consumer API with callbacks for rendering decoded video frames in a host application. \ndnrtcName{} builds on functionality provided by other libraries. NDN-CPP \cite{ndnccl} is used to access the NDN stack and to provide in-memory storage for the application. As discussed, the WebRTC framework \cite{webrtc} is used in two ways: 1) direct use of the video codec; 2) full incorporation of the audio pipeline, including echo cancellation. OpenFEC \cite{openfec} is used for forward error correction support. 

%Some features were incorporated into the library based on our experience in this application.
% Should discuss memory content cache?
%In most cases, consumers aim to express Interests for the data not yet produced, to be immediately satisfied when data is produced. The current NDN-CPP library provides a producer-side Memory Content Cache implementation into which data is published. However, this is only useful when data has been published and put in the cache before an Interest for this data has arrived. For the missing data, the Interest is forwarded to the producer application which stores it in the internal Pending Interests Table (PIT) unless requested data is ready. This functionality seems quite common for low-latency applications, and has now been incorporated into the NDN-CPP library implementation.

To demonstrate and evaluate the library, a desktop NDN videconferencing application, \ndnconName{}, \cite{ndncon} was implemented on top of \ndnrtcName{}. It provides a convenient user interface for publishing and fetching media streams, text chat, and organizing multi-party audio/video conferences. It was used, along with a command-line interface, for the evaluation below.
%% addressing review 60C-3
The \ndnrtcName{} library does not provide conference call setup functionality. This task was intentionally left out to be solved by applications that use it, and we are exploring it currently in \ndnconName{}.  The MacOS X platform is currently supported; Linux build instructions will be added soon. %The library distribution also comes with a simple console application which demonstrates the use of the \ndnrtcName{} library.

\section{Evaluation \& Iterative refinement}
\label{sec:eval} 
Over the course of \ndnrtcName{} development, numerous tests were run across the NDN testbed, as well as in isolated environments, to explore different library design patterns and implementations. These tests also helped us understand the nature of low-latency communication over NDN. We are still in the process of establishing well-defined metrics and test scenarios, but initial results generated refinements to our approach and are described below.
%During the course of \ndnrtcName{}'s initial development, 
There were several design iterations, and each introduced improvements in the overall quality of experience for the end-user, as well as in application efficiency related to bandwidth and computation. Each iteration tackled problems that were revealed during tests. These motivated namespace, application packet format and other revisions, which are reflected in the design detailed above.% and described further in this section. 

\subsection{Streaming performance}
%For the early versions of \ndnrtcName{} we've run multiple tests with 1 or more intermediate hubs in isolated testbed in order to achieve acceptable user experience for media streaming.

\textbf{Separation of key and delta frame namespaces.} Video streaming performance in early versions of \ndnrtcName{} suffered from video ``hiccups", even when being tested on trivial topologies. The cause of this problem turned out to be an inefficient frame fetching process. In early \ndnrtcName{} versions, the difference in size, and thus segments, of key frames and delta frames was not reflected in the producer's namespace, and consumers were forced to issue equal numbers of initial Interests ($M$), regardless of the frame type. This resulted in additional round trips of missing Interests and, consequently, larger assembling times ($d_{asm}$) for key frames that eventually led to missed playout deadlines and ``hiccup" effects. Having a separate namespace for key frames enables consumers to maintain separate Interest pipelines per frame type and collect historical data on the average number of Interests required to retrieve one frame of each type in one round trip. 
%A series of tests were conducted in order to assess bandwidth usage and perceived quality compared to Skype video calls. 

\textbf{Audio sample bundling.}
Another set of tests targeting streaming performance was conducted over the existing testbed with a number of volunteers from the NDN community. Apart from monitoring application performance, we gathered user feedback and compared the experience with Skype. 
Each test was comprised of six runs of two-person, five-minute conference calls using \ndnconName{}: a) three runs of audio+video with low, medium and high video bandwidths settings (0.5, 0.7 and 1.5 Mbit/s accordingly); b) one run of audio-only conference; c) one run of Skype audio+video conference; d) one run of Skype audio-only conference. Tests were conducted between the UCLA REMAP hub and six other hubs. These tests covered both one-hop and multi-hop paths. 
As a main outcome of these tests, audio sample bundling was quickly introduced in \ndnrtcName{} to reduce audio bandwidth (and the number of Interests on the consumer side), making it comparable to Skype audio bandwidths. Figure \ref{fig:tests-skype} shows overall bitrate usage results before audio bundling was implemented. As expected, Skype adapted to use link capacity between peers, and delivered higher bitrate videos; leaving such adaptive rate control as our highest priority future work. %We did not set comparison of performance and scalability between Skype and \ndnrtcName{} as our main goal and left it out for the future work. These kind of tests require clearly defined comparison framework between traditional and NDN low-latency applications.

% Obvious - 
%Actual average bitrates turned out to be slightly higher than pre-configured video streams -- 0.7, 0.9 and 1.8 Mbit/s for low, medium and high bandwidths respectively-- which can be explained by NDN packet overhead which is approximately 280-330 bytes and accounts for $\approx$30\% of the segment size (1000 bytes). 

% Obvious - 
%
%Having such large overhead makes transferring audio samples in separate segments highly inefficient. 

\begin{figure}[t!]
\centering
\begin{scriptsize}
\def\svgwidth{0.53\textwidth}\input{tests-skype.pdf_tex}
\end{scriptsize}
\vspace{-18pt}
\caption{Two-peer conference tests compared to Skype}
\label{fig:tests-skype}
\end{figure}

%The overall user experience for these 2-person conferences was subjectively assessed as being higher than average - 3 points on a scale from 0 to 5 (Skype calls were taken as 5-point user experiences).
%%% I would not include this. 

%The results of such testing influenced iterative updates to the design, two of which are described below. 

%Increased assembling time quite often caused skipping incomplete key frames, as they were not assembled by the time they should have been played out. Eventually, all the subsequent delta frames were skipped as well, which degraded the overall video streaming experience by introducing the video ``hiccup" effect. 
%%which made consumer to pipeline equal number of initial Interests--- this makes little sense to me, if any.
%% PG: tried to rephrase...

%\subsection{Quality of experience}
% PG: moved figure to the previous section


\subsection{Consumer-Producer synchronization}

\begin{figure}[t!]
\centering
\includegraphics[width=0.5\textwidth]{buffer}
\caption{Frame buffer}
\label{fig:old-buf}
\end{figure}


%%% -- This figure is not cited!

%\begin{figure}[t!]
%\centering
%
%\subfigure[$d_{gen} \approx 600ms$ resulted in 50\% retranmissions]{\includegraphics[width=0.4\textwidth]{dgen-bad}}
%\subfigure[$d_{gen} \approx 310ms$ - no redundant retransmissions]{\includegraphics[width=0.4\textwidth]{dgen-decent}}
%
%\caption{Two separate runs of earlier library version on similar topology (one-hop): random results for data generation delay $d_{gen}$ due to poor consumer-producer synchronization.}
%\label{fig:dgen}
%\end{figure}

\textbf{Bootstrap behavior.} In initial library versions based on the approach taken in NDNVideo, the consumer ``chased" the producer's time-series data by exhausting cached data via issuing a large number of outstanding Interests. However, there was no mechanism to adjust Interest expression dynamically; 
the buffering mechanism dictated the Interests' lifetime: All Interests entering the buffer had a lifetime equal to half of the current buffer size. Thus, it was expected that data will arrive before the Interest times out. In these cases, the Interest is re-issued, even with a half-buffer length remaining  to receive data before the playout deadline. This approach resulted in unavoidable Interest timeouts, in the cases when the consumer issued Interests far too early, before the actual data was produced.  This was further complicated by forwarding strategies in the NDN Forwarding Daemon (NFD) that did not handle consumer-initiated retransmission over short periods. 
For two similar test runs (one-hop topology), the number of timed out Interests and re-transmissions varied greatly (either $\approx$1\% or $\approx$50\%). %One was due to an incorrect consumer's synchronization with the producer; Interests were issued too early, so they timed out before any data had been produced. 
This problem was addressed by increasing the Interests' lifetimes\footnote{In fact, the dependence on Interests' lifetimes is not required anymore and every Interest is set to have 2000 ms lifetime.} and the introduction of a new NFD re-transmission strategy that allowed early Interests re-transmissions. 
Additionally, the re-transmission checkpoint is now placed at a time estimated to be the effective $RTT$ from the end of the buffer ($J=RTT$ on the Figure \ref{fig:old-buf}), which reflects a more accurate understanding of how data is received.
% This, together with an updated NFD re-transmission strategy \cite{nfd-rtx-release}, allows for larger Interests' lifetimes. 
%% addressing review 60D-2, 60D-8
%In fact, current implementation does not rely on Interests' lifetimes anymore and every Interest is set to have 2000 ms lifetime.

%%In the second sentence above, I changed it to until, but it was initially unless.

Moreover, the problem described above cannot occur if the consumer knows that it is issuing Interests too early. The chasing algorithm in older library versions was exhausting the network cache too aggressively; Interests were issued constantly until they filled up the buffer.  With the introduction of the $\lambda$ concept, the consumer exercises more precise control of the Interest expression as described previously. 
%Figure \ref{fig:ws} shows how a larger value of $\lambda$ helps to exhaust the cache more quickly. 
The number of outstanding Interests is controlled by a consumer and directly influences how fast consumer can ``chase" the producer. Thus, the consumer is able to control the ``agressiveness" of cache exhaustion and achieve a better synchronization state with the producer.
%% PG: removed these sentences as they duplicate "Interest expression control" section
%The $\lambda$ concept allows a ``lazy" start for the consumer. By specifying a smaller $\lambda$, the consumer issues less Interests. Further, the consumer observes cache exhaustion by monitoring $d_{arr}$ and, if the cache has not been exhausted during allocated time, the consumer may increase the value of $\lambda$ in order to express more Interests. Similarly, the consumer may opt to decrease $\lambda$ in cases where the original value resulted in behaviour that is too aggressive.

%\subsection{Test setup} 

%Include testbed topologies used, collaboration with WUSTL

%- one-hop
%- multiple-hops
%- many-to-many conferencing

%\subsection{Quality of experience} 

%How was the end-user quality of experience by the time the paper was written
%What kinds of things did you add to support it. 

\subsection{Multi-party use}

\begin{figure}[t!]
\centering

\subfigure[NDN testbed utilization during biweekly NDN seminar using \ndnconName{} for simulcast.]{\label{fig:one-to-many}\includegraphics[width=0.5\textwidth]{confbridge}}
\subfigure[NDN testbed utilization during 4-peer call between UCLA, REMAP, WUSTL and CAIDA hubs.]{\label{fig:many-to-many}\includegraphics[width=0.5\textwidth]{4peer}}

\caption{NDN testbed utilization during one-to-many and many-to-many scenarios.}
\label{fig:testbed-utilization}
\end{figure}

%Initial attempts to deploy the \ndnconName{} conferencing application were made in  2015. 
In another experiment, \ndnconName{} was used to stream an NDN seminar over the existing NDN testbed. An audio/video bridge was set up using third-party tools  allowing captured screen and audio feeds from existing IP-based conferencing tools to be simulcast. (Screen broadcast is now supported natively in later versions of \ndnconName{}.) Figure \ref{fig:one-to-many} shows an example of instantaneous NDN testbed utilization during the one-hour conference call. It is estimated that media streams were consumed by five to eight people. %Overall quality was satisfying, as reported by users.

Other tests of multi-party conferencing ability included four peers, each publishing three video streams and one audio stream and fetching one video and one audio stream from each of the other participants. Participants were distributed across four NDN testbed hubs - UCLA, REMAP, CAIDA and WUSTL, as shown in Figure \ref{fig:many-to-many}. Even though the user experience was satisfying and multi-peer conferences over NDN testbed have proven their viability, we plan more experimentation to explore quality of experience on a larger scale.
%These results were also generally satisfying from a quality of experience perspectives. However, for one user, audio cutoffs occurred more often than for the other participants. Many-to-many test scenarios are complex and are ongoing.
%%UCLA and REMAP? Or is there some clarification needed here?
%% PG: these are actually NDN testbed hubs, I've edited text

\section{Conclusion and Future Work}
\label{sec:conclusion}

\begin{figure}[t!]
\centering
\includegraphics[width=0.3\textwidth]{ndncon}
\caption{\ndnconName{} screenshot.}
\label{fig:ndncon}
\end{figure}

%% TODO: Conclusion

This paper presents the design, implementation, and initial experimental evaluation of \ndnrtcName{}, a library intended to support experimentation in real-time communications over Named Data Networking.   Our approach to this project has been experimentally driven so far, and has generated a functional low-latency streaming tool that we can now use as a platform for exploring important design challenges in real-time media over NDN.  This is a rich area, and some of the future work that we have identified includes: 
\begin{itemize}[label={}]

%% addressing reviews 60A-2, 60B-2, 60C-4, (60B-3, 60C-1)
\item \textbf{Scalability tests.} \ndnrtcName{} has shown that real-time communication using NDN is viable with the current open source implementation and on the current NDN testbed;  we are in the process of evaluating its performance in a variety of traffic scenarios and topologies. Our assumptions about the Interests and Data delivery patterns emerged from empirical observations of network behavior, and require more thorough experimentation in multi-peer scenarios, as well as simulation for much larger networks.  The best schemes for 1) determining the ``latest data'' the network can provide at the correct rate and 2) congestion control remain open challenges that require collaboration between application developers, architecture researchers, and testbed operators.

%We also acknowledge the need for wider scalability testing compared to existing IP solutions. %Our initial test results suggest that large-scale performance could be supported by NDN architecture inherently while delivering the same QoS as existing solutions.

\item \textbf{Adaptive rate control.} Multiple bitrate support is provided in the current design and implementation to lay the groundwork for adaptive rate control as a near-term effort, though for now the consuming application must manually select the best stream from bitrates offered by the producer.  In ongoing co-development of an adaptive rate control solution, we are exploring if monitoring of $d_{arr}$ and other approaches (described above) can address challenges of such adaptation over ICN, as suggested in papers such as \cite{posch2014client}.  

%% ** You should mention the Posch paper in related work and/or specifically mention here the client starvation problem. ** %% 

\item \textbf{Audio prioritization.}  For quality of experience in typical audio/videoconferencing applications, audio should be prioritized over video.  This can be done at the application level but may also benefit from architectural support. %, and we may explore such support in the future. % Others on the NDN team have proposed the notion of one-hop priority that would provide architecture support for relative prioritization of interests.

\item \textbf{Scalable video coding.} A more efficient way to relieve the producer from having to publish multiple copies of the same content at different bandwidths may be to use scalable video coding. By reflecting SVC layers in the namespace, the consumer will have more freedom for adapting media streams to the current network. Just as with audio, the SVC base layer may need to be prioritized; how to achieve this is an open question.

%% addressing review 60D-1
\item \textbf{Inter-consumer synchronization.} The absence of direct consumer-producer coordination shifted the complexity of ``RTC-over-NDN" streaming to the consumer. A related requirement of modern videoconference not covered by this work is to ensure media playback sychronization across different consumers.   This points more generally to the need for research on application-level time synchronization over NDN. 
%Synchronization primitives that were designed in ChronoChat \cite{chronochat} may become a good starting point in addressing this issue.

\item \textbf{Encryption-based access control.} The current \ndnrtcName{} design supports basic content signing and verification. However, a prominent requirement of most videoconferencing is confidentiality, which can be supported in NDN through encryption-based access control.  While encryption could limit the gains offered by caching, recent work exploring that application of advanced cryptographic techniques (such as attribute-based encryption to multimedia in ICN \cite{papanis2014use}) suggest new directions for meeting security requirements while leveraging key ICN features. 
%\item \textbf{Conference management} 
%Work on ndncon. unknown but verified publishers trust; 
%\end{itemize}
%%signatures consistency checks for successive media packets 
%% need to add the above before publication %%;  

\end{itemize}

\section{Acknowledgements}
\label{sec:Acknowledgements}
This project was partially supported by the National Science Foundation (award CNS-1345318 and others) and a grant from Cisco. The authors thank Lixia Zhang, Van Jacobson, and David Oran, as well as Eiichi Muramoto, Takahiro Yoneda, and Ryota Ohnishi, for their input and feedback. John DeHart, Josh Polterock, Jeff Thompson, Zhehao Wang and others on the NDN team provided invaluable testing of \ndnconName{}.  The initial forward error correction approach in \ndnrtcName{} was by Daisuke Ando. 

\newpage 

\bibliographystyle{abbrv}
{\small
\bibliography{bibliography}
}

\end{document}